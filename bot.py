import requests
import sys
import re
import logging
import os
import asyncio
import ast
import math
import html
import urllib.parse
from datetime import datetime, timedelta

from dotenv import load_dotenv
from functools import wraps

from telegram import Update
from telegram.constants import ChatAction, ParseMode
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes

# åŠ è½½.env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO,
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# ç¦æ­¢httpxçš„INFOçº§åˆ«æ—¥å¿—
httpx_logger = logging.getLogger("httpx")
httpx_logger.setLevel(logging.WARNING)

# --- ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½® ---
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
BASE_URL = os.getenv("ALIST_BASE_URL")
ALIST_TOKEN = os.getenv("ALIST_TOKEN")
ALIST_OFFLINE_DIRS = [d.strip() for d in os.getenv("ALIST_OFFLINE_DIRS", "").split(",") if d.strip()]
SEARCH_URL = os.getenv("JAV_SEARCH_API")
ALLOWED_USER_IDS_STR = os.getenv("ALLOWED_USER_IDS")
CLEAN_INTERVAL_MINUTES = int(os.getenv("CLEAN_INTERVAL_MINUTES", 60))
SIZE_THRESHOLD = int(os.getenv("SIZE_THRESHOLD", 100)) * 1024 * 1024

# --- é…ç½®æ ¡éªŒ ---
if not all([TELEGRAM_TOKEN, BASE_URL, ALIST_TOKEN, ALIST_OFFLINE_DIRS, SEARCH_URL, ALLOWED_USER_IDS_STR]):
    logger.error("é”™è¯¯ï¼šç¯å¢ƒå˜é‡ç¼ºå¤±ï¼è¯·æ£€æŸ¥.env æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡è®¾ç½®ã€‚")
    sys.exit(1)
if not ALIST_OFFLINE_DIRS:
    logger.error("é”™è¯¯ï¼šæœªé…ç½®ä»»ä½•ä¸‹è½½è·¯å¾„ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®ALIST_OFFLINE_DIRS")
    sys.exit(1)

try:
    ALLOWED_USER_IDS = set(map(int, ALLOWED_USER_IDS_STR.split(',')))
    logger.info(f"å…è®¸çš„ç”¨æˆ· ID: {ALLOWED_USER_IDS}")
    logger.info(f"Loaded download directories: {ALIST_OFFLINE_DIRS}")
except ValueError:
    logger.error("é”™è¯¯: ALLOWED_USER_IDS æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·ç¡®ä¿æ˜¯é€—å·åˆ†éš”çš„æ•°å­—ã€‚")
    sys.exit(1)

# --- ç”¨æˆ·æˆæƒè£…é¥°å™¨ ---
def restricted(func):
    @wraps(func)
    async def wrapped(update: Update, context: ContextTypes.DEFAULT_TYPE, *args, **kwargs):
        user_id = update.effective_user.id
        if user_id not in ALLOWED_USER_IDS:
            logger.warning(f"æœªæˆæƒç”¨æˆ·å°è¯•è®¿é—®: {user_id}")
            await update.message.reply_text("æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
            return
        return await func(update, context, token=ALIST_TOKEN, *args, **kwargs)
    return wrapped

# --- API å‡½æ•° ---

def parse_size_to_bytes(size_str: str) -> int | None:
    """Converts size string (e.g., '5.40GB', '1.25MB') to bytes."""
    if not size_str:
        return 0

    size_str = size_str.upper()
    match = re.match(r'^([\d.]+)\s*([KMGTPEZY]?B)$', size_str)
    if not match:
        logger.warning(f"æ— æ³•è§£ææ–‡ä»¶å¤§å°: {size_str}")
        return None

    value, unit = match.groups()
    try:
        value = float(value)
    except ValueError:
        logger.warning(f"æ— æ³•è§£ææ–‡ä»¶å¤§å°å€¼: {value} from {size_str}")
        return None

    unit = unit.upper()
    exponent = 0
    if unit.startswith('K'):
        exponent = 1
    elif unit.startswith('M'):
        exponent = 2
    elif unit.startswith('G'):
        exponent = 3
    elif unit.startswith('T'):
        exponent = 4

    return int(value * (1024 ** exponent))

def parse_api_data_entry(entry_str: str) -> dict | None:
    """Parses a single string entry from the API data list."""
    try:
        data_list = ast.literal_eval(entry_str)
        if not isinstance(data_list, list) or len(data_list) < 4:
            logger.warning(f"è§£æåçš„æ•°æ®æ ¼å¼ä¸æ­£ç¡® (éåˆ—è¡¨æˆ–é•¿åº¦ä¸è¶³): {data_list}")
            return None

        magnet = data_list[0]
        name = data_list[1]
        size_str = data_list[2]
        date_str = data_list[3]

        if not magnet or not magnet.startswith("magnet:?"):
            logger.warning(f"æ¡ç›®ä¸­ç¼ºå°‘æœ‰æ•ˆçš„ç£åŠ›é“¾æ¥: {entry_str}")
            return None

        size_bytes = parse_size_to_bytes(size_str)
        if size_bytes is None:
            logger.warning(f"æ— æ³•è§£æå¤§å°ï¼Œè·³è¿‡æ¡ç›®: {entry_str}")
            return None

        upload_date = None
        try:
            if date_str:
                upload_date = datetime.strptime(date_str, '%Y-%m-%d').date()
        except ValueError:
            logger.warning(f"æ— æ³•è§£ææ—¥æœŸ '{date_str}'ï¼Œæ—¥æœŸå°†ä¸º None")

        return {
            "magnet": magnet,
            "name": name,
            "size_str": size_str,
            "size_bytes": size_bytes,
            "date_str": date_str,
            "date": upload_date,
            "original_string": entry_str
        }
    except (ValueError, SyntaxError, TypeError) as e:
        logger.error(f"è§£æ API æ•°æ®æ¡ç›®æ—¶å‡ºé”™: '{entry_str[:100]}...', é”™è¯¯: {e}")
        return None

def get_magnet(fanhao: str, search_url: str) -> tuple[str | None, str | None]:
    """è·å–ç£åŠ›é“¾æ¥"""
    try:
        url = search_url.rstrip('/') + "/" + fanhao
        logger.info(f"æ­£åœ¨æœç´¢ç•ªå·: {fanhao}")
        response = requests.get(url, timeout=20)
        response.raise_for_status()

        raw_result = response.json()

        if not raw_result or raw_result.get("status") != "succeed":
            error_type = raw_result.get('message', 'æœªçŸ¥é”™è¯¯')
            if "not found" in error_type.lower():
                return None, f"ğŸ” æœªæ‰¾åˆ°ç•ªå· {fanhao} ç›¸å…³èµ„æº"
            return None, f"ğŸ” æœç´¢æœåŠ¡å¼‚å¸¸ ({error_type[:20]}...)"

        if not raw_result.get("data") or len(raw_result["data"]) == 0:
            return None, f"ğŸ” ç•ªå· {fanhao} æš‚æ— æœ‰æ•ˆç£åŠ›"

        parsed_entries = []
        for entry_str in raw_result["data"]:
            parsed = parse_api_data_entry(entry_str)
            if parsed and parsed["magnet"].startswith("magnet:?"):
                parsed_entries.append(parsed)

        if not parsed_entries:
            return None, f"ğŸ” æ‰¾åˆ°èµ„æºä½†æ— æœ‰æ•ˆç£åŠ›"

        max_size = max(e["size_bytes"] for e in parsed_entries)
        hd_threshold = max_size * 0.7
        selected_cluster = [e for e in parsed_entries if e["size_bytes"] >= hd_threshold] or parsed_entries

        selected_cluster.sort(
            key=lambda x: (x["size_bytes"],
                         -(x["date"].toordinal() if x["date"] else 0)))

        return selected_cluster[0]["magnet"], None
    except requests.exceptions.Timeout:
        logger.error(f"æœç´¢è¶…æ—¶ ({fanhao})")
        return None, "â³ æœç´¢è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
    except requests.exceptions.HTTPError as e:
        status_code = e.response.status_code
        if 500 <= status_code < 600 or status_code == 404:
            return None, f"ğŸ” ç•ªå· {fanhao} ä¸å­˜åœ¨"
        return None, f"ğŸ” æœç´¢æœåŠ¡å¼‚å¸¸ (HTTP {status_code})"
    except Exception as e:
        logger.error(f"æœªçŸ¥é”™è¯¯ ({fanhao}): {str(e)}", exc_info=True)
        if "timed out" in str(e).lower():
            return None, "â³ æ“ä½œè¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
        return None, "ğŸ” æœç´¢æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯"

async def add_magnet(context: ContextTypes.DEFAULT_TYPE, token: str, magnet: str) -> tuple[bool, str]:
    """æ·»åŠ ç£åŠ›é“¾æ¥åˆ° Alist"""
    if not token or not magnet:
        logger.error("æ·»åŠ ä»»åŠ¡å¤±è´¥: token æˆ–ç£åŠ›é“¾æ¥ä¸ºç©º")
        return False, "âŒ å†…éƒ¨é”™è¯¯ï¼šå¿…è¦å‚æ•°ç¼ºå¤±"

    try:
        url = BASE_URL.rstrip('/') + "/api/fs/add_offline_download"
        headers = {
            "Authorization": token,
            "Content-Type": "application/json"
        }
        post_data = {
            "path": context.bot_data.get('current_download_dir', ALIST_OFFLINE_DIRS[0]),
            "urls": [magnet],
            "tool": "storage",
            "delete_policy": "delete_on_upload_succeed"
        }

        loop = asyncio.get_running_loop()
        response = await loop.run_in_executor(
            None, lambda: requests.post(url, json=post_data, headers=headers, timeout=30))

        if response.status_code == 401:
            return False, "âŒ è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ ALIST_TOKEN"
        if response.status_code == 500:
            return False, "âŒ æœåŠ¡å™¨æ‹’ç»è¯·æ±‚ï¼ˆå¯èƒ½é‡å¤æ·»åŠ ï¼‰"

        response.raise_for_status()
        result = response.json()

        if result.get("code") == 200:
            return True, "âœ… å·²æ·»åŠ è‡³ä¸‹è½½é˜Ÿåˆ—"
        return False, f"âŒ ç£åŠ›è§£æå¤±è´¥"
    except requests.exceptions.Timeout:
        return False, "â³ æ·»åŠ è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ"
    except requests.exceptions.ConnectionError:
        return False, "ğŸ”Œ æ— æ³•è¿æ¥AlistæœåŠ¡"
    except Exception as e:
        logger.error(f"æ·»åŠ ä»»åŠ¡å¼‚å¸¸: {str(e)}")
        return False, f"âŒ æ„å¤–é”™è¯¯: {str(e)[:50]}"

async def recursive_collect_files(token: str, base_url: str, current_path: str) -> list[str]:
    """é€’å½’æ”¶é›†ç›®å½•ä¸‹æ‰€æœ‰å°æ–‡ä»¶"""
    if SIZE_THRESHOLD == 0:
        return []
    list_url = base_url.rstrip('/') + "/api/fs/list"
    headers = {"Authorization": token, "Content-Type": "application/json"}
    payload = {"path": current_path, "page": 1, "per_page": 0}
    files = []

    try:
        loop = asyncio.get_running_loop()
        response = await loop.run_in_executor(
            None, lambda: requests.post(list_url, json=payload, headers=headers, timeout=20)
        )
        response.raise_for_status()
        list_result = response.json()

        data = list_result.get("data") or {}
        if list_result.get("code") != 200:
            logger.error(f"ç›®å½•åˆ—è¡¨å¤±è´¥: {list_result.get('message')} (è·¯å¾„: {current_path})")
            return []

        content = data.get("content") or []
        if not isinstance(content, list):
            logger.error(f"æ— æ•ˆçš„APIå“åº”æ ¼å¼ (è·¯å¾„: {current_path})")
            return []

        for item in content:
            try:
                is_dir = item.get("is_dir", False)
                file_name = item.get("name", "").strip()
                file_size = item.get("size", 0)

                if not file_name:
                    continue

                full_path = "/".join([current_path.rstrip("/"), file_name.lstrip("/")])

                if is_dir:
                    sub_files = await recursive_collect_files(token, base_url, full_path)
                    files.extend(sub_files)
                else:
                    if file_size < SIZE_THRESHOLD:
                        files.append(full_path)
                        logger.debug(f"æ‰¾åˆ°å€™é€‰æ–‡ä»¶: {full_path} ({file_size/1024/1024:.2f} MB)")
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶é¡¹æ—¶å‡ºé”™: {str(e)}", exc_info=True)
                continue

        return files
    except requests.exceptions.RequestException as e:
        logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)} (è·¯å¾„: {current_path})")
        return []
    except Exception as e:
        logger.error(f"æœªçŸ¥é”™è¯¯: {str(e)} (è·¯å¾„: {current_path})", exc_info=True)
        return []

async def recursive_collect_empty_dirs(token: str, base_url: str, current_path: str) -> list[str]:
    """é€’å½’æ”¶é›†ç©ºæ–‡ä»¶å¤¹"""
    list_url = base_url.rstrip('/') + "/api/fs/list"
    headers = {"Authorization": token, "Content-Type": "application/json"}
    payload = {"path": current_path, "page": 1, "per_page": 0}
    empty_dirs = []

    try:
        loop = asyncio.get_running_loop()
        response = await loop.run_in_executor(
            None, lambda: requests.post(list_url, json=payload, headers=headers, timeout=20)
        )
        response.raise_for_status()
        list_result = response.json()

        data = list_result.get("data") or {}
        if list_result.get("code") != 200:
            logger.error(f"ç›®å½•åˆ—è¡¨å¤±è´¥: {list_result.get('message')} (è·¯å¾„: {current_path})")
            return []

        content = data.get("content") or []
        if not isinstance(content, list):
            logger.error(f"æ— æ•ˆçš„APIå“åº”æ ¼å¼ (è·¯å¾„: {current_path})")
            return []

        sub_dirs = []
        for item in content:
            is_dir = item.get("is_dir", False)
            file_name = item.get("name", "").strip()
            if not file_name:
                continue
            full_path = "/".join([current_path.rstrip("/"), file_name.lstrip("/")])
            if is_dir:
                sub_dirs.append(full_path)
        for sub_dir in sub_dirs:
            sub_empty_dirs = await recursive_collect_empty_dirs(token, base_url, sub_dir)
            empty_dirs.extend(sub_empty_dirs)

        if not sub_dirs and not any(not item.get("is_dir", False) for item in content):
            empty_dirs.append(current_path)

        return empty_dirs
    except requests.exceptions.RequestException as e:
        logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)} (è·¯å¾„: {current_path})")
        return []
    except Exception as e:
        logger.error(f"æœªçŸ¥é”™è¯¯: {str(e)} (è·¯å¾„: {current_path})", exc_info=True)
        return []

async def cleanup_empty_dirs(token: str, base_url: str, target_dir: str) -> tuple[int, str]:
    """æ¸…ç†ç©ºæ–‡ä»¶å¤¹"""
    try:
        empty_dirs = await recursive_collect_empty_dirs(token, base_url, target_dir)
        if not empty_dirs:
            return 0, "âœ… æœªæ‰¾åˆ°ç©ºæ–‡ä»¶å¤¹"

        total_deleted = 0
        error_messages = []
        for dir_path in empty_dirs:
            try:
                remove_url = base_url.rstrip('/') + "/api/fs/remove"
                headers = {"Authorization": token, "Content-Type": "application/json"}
                delete_payload = {
                    "dir": os.path.dirname(dir_path),
                    "names": [os.path.basename(dir_path)]
                }
                response = requests.post(remove_url, json=delete_payload, headers=headers, timeout=30)
                if response.status_code == 200:
                    result = response.json()
                    if result.get("code") == 200:
                        total_deleted += 1
                        logger.debug(f"æˆåŠŸåˆ é™¤ç©ºæ–‡ä»¶å¤¹: {dir_path}")
                    else:
                        error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
                        error_messages.append(f"æ–‡ä»¶å¤¹ {os.path.basename(dir_path)}: {error_msg}")
                else:
                    response.raise_for_status()
            except requests.exceptions.RequestException as e:
                error_messages.append(f"æ–‡ä»¶å¤¹ {os.path.basename(dir_path)}: ç½‘ç»œé”™è¯¯")
            except Exception as e:
                error_messages.append(f"æ–‡ä»¶å¤¹ {os.path.basename(dir_path)}: {str(e)}")

        if error_messages:
            return total_deleted, (
                f"âŒ éƒ¨åˆ†åˆ é™¤å¤±è´¥ (æˆåŠŸ {total_deleted} æ–‡ä»¶å¤¹)\n"
                f"é”™è¯¯({min(len(error_messages), 3)}/{len(error_messages)}):\n" +
                "\n".join([f"â€¢ {msg}" for msg in error_messages[:3]])
            )
        return total_deleted, f"âœ… æˆåŠŸåˆ é™¤ {total_deleted} ä¸ªç©ºæ–‡ä»¶å¤¹"
    except Exception as e:
        logger.error(f"æ¸…ç†ç©ºæ–‡ä»¶å¤¹å¼‚å¸¸: {str(e)}", exc_info=True)
        return 0, f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}"

async def cleanup_small_files(token: str, base_url: str, target_dir: str) -> tuple[int, str]:
    """æ¸…ç†å°æ–‡ä»¶"""
    if SIZE_THRESHOLD == 0:
        return 0, "âœ… å°æ–‡ä»¶æ¸…ç†åŠŸèƒ½æœªå¯ç”¨"
    try:
        from collections import defaultdict
        from urllib.parse import quote

        logger.info(f"å¼€å§‹æ¸…ç†ç›®å½•: {target_dir}")
        files_to_delete = await recursive_collect_files(token, base_url, target_dir)

        if not files_to_delete:
            return 0, "âœ… æœªæ‰¾åˆ°å°äºæŒ‡å®šå¤§å°çš„æ–‡ä»¶"

        dir_files = defaultdict(list)
        for abs_path in files_to_delete:
            parent_dir = os.path.dirname(abs_path)
            file_name = os.path.basename(abs_path)
            dir_files[parent_dir].append(file_name)

        total_deleted_files = 0
        file_error_messages = []

        for parent_dir, file_names in dir_files.items():
            try:
                remove_url = base_url.rstrip('/') + "/api/fs/remove"
                headers = {"Authorization": token, "Content-Type": "application/json"}
                delete_payload = {"dir": parent_dir, "names": file_names}
                response = requests.post(remove_url, json=delete_payload, headers=headers, timeout=30)

                if response.status_code == 200:
                    result = response.json()
                    if result.get("code") == 200:
                        deleted = len(file_names)
                        total_deleted_files += deleted
                        logger.debug(f"æˆåŠŸåˆ é™¤ {deleted} ä¸ªæ–‡ä»¶äº {parent_dir}")
                    else:
                        error_msg = f"ç›®å½• {os.path.basename(parent_dir)}: API è¿”å›é”™è¯¯ç  {result.get('code')}ï¼Œæ¶ˆæ¯: {result.get('message')}"
                        file_error_messages.append(error_msg)
                else:
                    response.raise_for_status()
            except requests.exceptions.RequestException as e:
                error_msg = f"ç›®å½• {os.path.basename(parent_dir)}: ç½‘ç»œé”™è¯¯ - {str(e)}"
                file_error_messages.append(error_msg)
            except Exception as e:
                error_msg = f"ç›®å½• {os.path.basename(parent_dir)}: {str(e)}"
                file_error_messages.append(error_msg)

        total_deleted_dirs, dir_msg = await cleanup_empty_dirs(token, base_url, target_dir)

        if file_error_messages and total_deleted_files == 0 and total_deleted_dirs == 0:
            return 0, (
                f"âŒ éƒ¨åˆ†åˆ é™¤å¤±è´¥ (æˆåŠŸ 0 ä¸ªæ–‡ä»¶å’Œ 0 ä¸ªç©ºæ–‡ä»¶å¤¹)\n"
                f"é”™è¯¯({min(len(file_error_messages), 3)}/{len(file_error_messages)}):\n" +
                "\n".join([f"â€¢ {msg}" for msg in file_error_messages[:3]])
            )

        if total_deleted_files > 0 or total_deleted_dirs > 0:
            if total_deleted_dirs > 0:
                if file_error_messages:
                    return total_deleted_files, (
                        f"âœ… éƒ¨åˆ†æ–‡ä»¶åˆ é™¤å¤±è´¥ï¼Œä½†æˆåŠŸåˆ é™¤ {total_deleted_files} ä¸ªæ–‡ä»¶å’Œ {total_deleted_dirs} ä¸ªç©ºæ–‡ä»¶å¤¹\n"
                        f"æ–‡ä»¶åˆ é™¤é”™è¯¯({min(len(file_error_messages), 3)}/{len(file_error_messages)}):\n" +
                        "\n".join([f"â€¢ {msg}" for msg in file_error_messages[:3]])
                    )
                else:
                    return total_deleted_files, f"âœ… æˆåŠŸåˆ é™¤ {total_deleted_files} ä¸ªæ–‡ä»¶å’Œ {total_deleted_dirs} ä¸ªç©ºæ–‡ä»¶å¤¹"
            else:
                if file_error_messages:
                    return total_deleted_files, (
                        f"âœ… éƒ¨åˆ†æ–‡ä»¶åˆ é™¤å¤±è´¥ï¼Œä½†æˆåŠŸåˆ é™¤ {total_deleted_files} ä¸ªæ–‡ä»¶\n"
                        f"æ–‡ä»¶åˆ é™¤é”™è¯¯({min(len(file_error_messages), 3)}/{len(file_error_messages)}):\n" +
                        "\n".join([f"â€¢ {msg}" for msg in file_error_messages[:3]])
                    )
                else:
                    return total_deleted_files, f"âœ… æˆåŠŸåˆ é™¤ {total_deleted_files} ä¸ªæ–‡ä»¶ã€‚{dir_msg}"
        else:
            return 0, f"âœ… æœªæ‰¾åˆ°å°äºæŒ‡å®šå¤§å°çš„æ–‡ä»¶ï¼Œ{dir_msg}"
    except Exception as e:
        logger.error(f"æ¸…ç†å¼‚å¸¸: {str(e)}", exc_info=True)
        return 0, f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}"

async def find_download_directory(token: str, base_url: str, parent_dir: str, original_code: str) -> tuple[list[str] | None, str | None]:
    """æŸ¥æ‰¾åŒ¹é…çš„ç›®å½•"""
    logger.info(f"åœ¨ç›®å½• '{parent_dir}' ä¸­æœç´¢ç•ªå· '{original_code}'...")
    list_url = base_url.rstrip('/') + "/api/fs/list"
    headers = {"Authorization": token, "Content-Type": "application/json"}

    try:
        parent_dir = parent_dir.strip().replace('\\', '/').rstrip('/')
        if not parent_dir.startswith('/'):
            parent_dir = f'/{parent_dir}'

        list_payload = {"path": parent_dir, "page": 1, "per_page": 0}
        response = requests.post(list_url, json=list_payload, headers=headers, timeout=20)
        response.raise_for_status()
        list_result = response.json()

        if list_result.get("code") != 200:
            return None, f"ç›®å½•åˆ—è¡¨å¤±è´¥: {list_result.get('message', 'æœªçŸ¥é”™è¯¯')}"

        content = list_result.get("data", {}).get("content", [])
        target_pattern = re.sub(r'[^a-zA-Z0-9]', '', original_code).lower()
        possible_matches = []

        for item in content:
            if item.get("is_dir"):
                dir_name = item.get("name", "").strip()
                normalized_dir = re.sub(r'[^a-zA-Z0-9]', '', dir_name).lower()
                if normalized_dir.startswith(target_pattern):
                    full_path = f"{parent_dir.rstrip('/')}/{dir_name}".replace('//', '/')
                    possible_matches.append(full_path)
                    logger.debug(f"æ‰¾åˆ°å€™é€‰ç›®å½•: {full_path}")

        return possible_matches, None
    except Exception as e:
        logger.error(f"ç›®å½•æœç´¢å¼‚å¸¸: {str(e)}")
        return None, f"ç›®å½•æœç´¢å¤±è´¥: {str(e)}"

# --- Telegram å‘½ä»¤å¤„ç†å‡½æ•° ---

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user_id = update.effective_user.id
    if user_id not in ALLOWED_USER_IDS:
        await update.message.reply_text("æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
        return
    await update.message.reply_text(
        'æ¬¢è¿ä½¿ç”¨ JAV ä¸‹è½½æœºå™¨äººï¼\n'
        'ç›´æ¥å‘é€ç•ªå·ï¼ˆå¦‚ ABC-123ï¼‰æˆ–ç£åŠ›é“¾æ¥ï¼Œæˆ‘ä¼šå¸®ä½ æ·»åŠ åˆ° Alist ç¦»çº¿ä¸‹è½½ã€‚\n'
        '/help æŸ¥çœ‹å¸®åŠ©ã€‚'
    )

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user_id = update.effective_user.id
    if user_id not in ALLOWED_USER_IDS:
        await update.message.reply_text("æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
        return
    await update.message.reply_text(
        'ä½¿ç”¨æ–¹æ³•ï¼š\n'
        '1. ç›´æ¥å‘é€ç•ªå·ï¼ˆä¾‹å¦‚ï¼š`ABC-123`, `IPX-888`ï¼‰\n'
        '2. ç›´æ¥å‘é€ç£åŠ›é“¾æ¥ï¼ˆä»¥ `magnet:?` å¼€å¤´ï¼‰\n\n'
        '3. æ¸…ç†åŠŸèƒ½ï¼š\n'
        '   - `/clean <ç•ªå·>` æ¸…ç†è¯¥ç•ªå·å¯¹åº”çš„ä¸‹è½½ç›®å½•\n'
        '   - `/clean /` é€’å½’æ¸…ç†å½“å‰ä¸‹è½½ç›®å½•\n\n'
        '4. åˆ·æ–°åŠŸèƒ½ï¼š\n'
        '   - `/refresh` åˆ·æ–° Alist æ–‡ä»¶åˆ—è¡¨\n\n'
        '5. ç®¡ç†ä¸‹è½½ç›®å½•ï¼š\n'
        '   - `/list_paths` åˆ—å‡ºæ‰€æœ‰å¯ç”¨ä¸‹è½½ç›®å½•\n'
        '   - `/switch <number>` åˆ‡æ¢åˆ°æŒ‡å®šçš„ä¸‹è½½ç›®å½•ï¼ˆä¾‹å¦‚ /switch 2ï¼‰\n'
        '   - `/reload_config` é‡æ–°åŠ è½½é…ç½®ï¼ˆåŒ…æ‹¬ä¸‹è½½ç›®å½•ï¼‰\n\n'
        f'å½“å‰ä¸‹è½½æ ¹ç›®å½•: `{context.bot_data.get("current_download_dir", "æœªçŸ¥")}`',
        parse_mode='Markdown'
    )

FANHAO_REGEX = re.compile(
    r'^[A-Za-z]{2,5}[-_ ]?\d{2,5}(?:[-_ ]?[A-Za-z])?$',
    re.IGNORECASE
)

async def handle_single_entry(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str, entry: str):
    chat_id = update.effective_chat.id
    loop = asyncio.get_event_loop()
    processing_msg = None

    try:
        if entry.startswith("magnet:?"):
            logger.info(f"æ”¶åˆ°ç£åŠ›é“¾æ¥: {entry[:50]}...")
            processing_msg = await update.message.reply_text("ğŸ”— æ”¶åˆ°ç£åŠ›é“¾æ¥ï¼Œå‡†å¤‡æ·»åŠ ...")
            success, result_msg = await add_magnet(context, token, entry)
        elif FANHAO_REGEX.match(entry):
            logger.info(f"æ”¶åˆ°å¯èƒ½çš„ç•ªå·: {entry}")
            processing_msg = await update.message.reply_text(f"ğŸ” æ­£åœ¨æœç´¢ç•ªå·: {entry}...")
            await context.bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)

            magnet, error_msg = await loop.run_in_executor(
                None, lambda: get_magnet(entry, SEARCH_URL)
            )

            if not magnet:
                await processing_msg.edit_text(f"âŒ æœç´¢å¤±è´¥: {error_msg}")
                return

            await processing_msg.edit_text(f"âœ… å·²æ‰¾åˆ°ç£åŠ›é“¾æ¥ï¼Œæ­£åœ¨æ·»åŠ åˆ° Alist...")
            success, result_msg = await add_magnet(context, token, magnet)
        else:
            await update.message.reply_text("æ— æ³•è¯†åˆ«çš„æ¶ˆæ¯æ ¼å¼ã€‚è¯·å‘é€ç•ªå·ï¼ˆå¦‚ ABC-123ï¼‰æˆ–ç£åŠ›é“¾æ¥ã€‚")
            return

        if processing_msg:
            await processing_msg.edit_text(result_msg)
        else:
            await update.message.reply_text(result_msg)

        if success:
            await asyncio.sleep(3)
            await refresh_command(update, context)
    except Exception as e:
        logger.error(f"å¤„ç†å¼‚å¸¸: {str(e)}", exc_info=True)
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)[:100]}"
        if processing_msg:
            await processing_msg.edit_text(error_msg)
        else:
            await update.message.reply_text(error_msg)

async def handle_batch_entries(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str, entries: list[str]):
    chat_id = update.effective_chat.id
    loop = asyncio.get_event_loop()
    progress_msg = await update.message.reply_text(f"ğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç† {len(entries)} ä¸ªä»»åŠ¡...")
    results = []
    BATCH_DELAY = 0.8

    for idx, entry in enumerate(entries, 1):
        try:
            success_count = sum(1 for res in results if res[1])
            await progress_msg.edit_text(
                f"â³ å¤„ç†è¿›åº¦: {idx}/{len(entries)}\n"
                f"å½“å‰é¡¹: {entry[:15]}...\n"
                f"æˆåŠŸ: {success_count} å¤±è´¥: {len(results)-success_count}"
            )

            if entry.startswith("magnet:?"):
                success, msg = await add_magnet(context, token, entry)
                results.append((entry, success, msg))
            elif FANHAO_REGEX.match(entry):
                magnet, error = await loop.run_in_executor(None, lambda: get_magnet(entry, SEARCH_URL))
                if magnet:
                    success, msg = await add_magnet(context, token, magnet)
                    results.append((entry, success, msg))
                else:
                    results.append((entry, False, f"æœç´¢å¤±è´¥: {error}"))
            else:
                results.append((entry, False, "æ ¼å¼é”™è¯¯"))

            await asyncio.sleep(BATCH_DELAY)
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†å¼‚å¸¸: {entry} - {str(e)}")
            results.append((entry, False, f"å¤„ç†å¼‚å¸¸: {str(e)[:50]}"))
            await asyncio.sleep(BATCH_DELAY * 2)

    success_count = sum(1 for res in results if res[1])
    report = [
        f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ ({success_count}/{len(entries)})",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
        *[f"{'ğŸŸ¢' if res[1] else 'ğŸ”´'} {res[0][:20]}... | {res[2][:30]}"
          for res in results[:10]],
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
        f"æˆåŠŸ: {success_count} æ¡ | å¤±è´¥: {len(entries)-success_count} æ¡"
    ]
    if len(results) > 10:
        report.insert(3, f"ï¼ˆä»…æ˜¾ç¤ºå‰10æ¡ç»“æœï¼Œå…±{len(entries)}æ¡ï¼‰")

    await progress_msg.edit_text("\n".join(report))
    await context.bot.send_message(
        chat_id=chat_id,
        text="ğŸ’¡ æç¤ºï¼šä½¿ç”¨ /clean / å‘½ä»¤å¯ä»¥æ¸…ç†æ‰€æœ‰åƒåœ¾æ–‡ä»¶",
        reply_to_message_id=progress_msg.message_id
    )

    if success_count > 0:
        await asyncio.sleep(3)
        await refresh_command(update, context)

@restricted
async def process_message(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str) -> None:
    message_text = update.message.text.strip()
    entries = [line.strip() for line in message_text.split('\n') if line.strip()]
    if not entries:
        await update.message.reply_text("âš ï¸ è¾“å…¥å†…å®¹ä¸ºç©ºï¼Œè¯·å‘é€ç•ªå·æˆ–ç£åŠ›é“¾æ¥ã€‚")
        return

    if len(entries) == 1:
        await handle_single_entry(update, context, token, entries[0])
    else:
        await handle_batch_entries(update, context, token, entries)

@restricted
async def clean_command(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str) -> None:
    """æ¸…ç†åŒ¹é…ç›®å½•"""
    if SIZE_THRESHOLD == 0:
        await update.message.reply_text("âœ… å°æ–‡ä»¶æ¸…ç†åŠŸèƒ½æœªå¯ç”¨")
        return
    if not context.args:
        await update.message.reply_text("è¯·æä¾›æ¸…ç†å‚æ•°ï¼š/clean <ç•ªå·> æˆ– /clean /")
        return

    target = context.args[0].strip()
    chat_id = update.effective_chat.id
    processing_msg = await update.message.reply_text(f"ğŸ§¹ å¼€å§‹æ¸…ç†ä»»åŠ¡ï¼ˆç›®æ ‡: {target}ï¼‰...")

    try:
        current_dir = context.bot_data.get('current_download_dir', ALIST_OFFLINE_DIRS[0])
        if target == "/":
            deleted_files, msg = await cleanup_small_files(token, BASE_URL, current_dir)
            final_text = f"å…¨å±€æ¸…ç†å®Œæˆ\n{msg}"
            await processing_msg.edit_text(final_text)
            return

        directories, find_error = await find_download_directory(token, BASE_URL, current_dir, target)
        if not directories:
            await processing_msg.edit_text(f"âŒ æ¸…ç†å¤±è´¥: {find_error}")
            return

        logger.info(f"æ‰¾åˆ° {len(directories)} ä¸ªåŒ¹é…ç›®å½•ï¼Œå¼€å§‹æ‰¹é‡æ¸…ç†...")
        success_dirs = 0
        total_files = 0
        total_dirs = len(directories)
        error_messages = []

        for idx, dir_path in enumerate(directories, 1):
            await processing_msg.edit_text(
                f"ğŸ§¹ æ­£åœ¨æ¸…ç† ({idx}/{total_dirs}): {os.path.basename(dir_path)}..."
            )
            deleted, msg = await cleanup_small_files(token, BASE_URL, dir_path)
            if deleted > 0:
                success_dirs += 1
                total_files += deleted
            if 'âŒ' in msg:
                error_messages.append(msg)

        zero_dirs_count = total_dirs - success_dirs - len(error_messages)
        if success_dirs > 0 and zero_dirs_count == 0 and len(error_messages) == 0:
            final_text = f"âœ… æ¸…ç†å®Œæˆï¼å…±æ¸…ç† {total_files} ä¸ªå°æ–‡ä»¶ï¼Œæ¶‰åŠ {success_dirs} ä¸ªç›®å½•ã€‚"
        elif success_dirs > 0 and zero_dirs_count == 0 and len(error_messages) > 0:
            final_text = (
                f"âœ… éƒ¨åˆ†æ¸…ç†å®Œæˆï¼æˆåŠŸæ¸…ç† {total_files} ä¸ªå°æ–‡ä»¶ï¼Œæ¶‰åŠ {success_dirs} ä¸ªç›®å½•ã€‚\n"
                f"âŒ ä»¥ä¸‹ç›®å½•æ¸…ç†å¤±è´¥ ({len(error_messages)}):\n" +
                "\n".join([f"â€¢ {msg}" for msg in error_messages[:3]])
            )
        elif success_dirs > 0 and zero_dirs_count > 0 and len(error_messages) == 0:
            final_text = (
                f"âœ… éƒ¨åˆ†æ¸…ç†å®Œæˆï¼æˆåŠŸæ¸…ç† {total_files} ä¸ªå°æ–‡ä»¶ï¼Œæ¶‰åŠ {success_dirs} ä¸ªç›®å½•ã€‚\n"
                f"âš ï¸ ä»¥ä¸‹ç›®å½•æœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„æ–‡ä»¶ ({zero_dirs_count}):\n" +
                "\n".join([os.path.basename(d) for d in directories if d not in [d for _, msg in zip(directories, msg) if 'âœ…' in msg]])
            )
        elif success_dirs > 0 and zero_dirs_count > 0 and len(error_messages) > 0:
            final_text = (
                f"âœ… éƒ¨åˆ†æ¸…ç†å®Œæˆï¼æˆåŠŸæ¸…ç† {total_files} ä¸ªå°æ–‡ä»¶ï¼Œæ¶‰åŠ {success_dirs} ä¸ªç›®å½•ã€‚\n"
                f"âš ï¸ ä»¥ä¸‹ç›®å½•æœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„æ–‡ä»¶ ({zero_dirs_count}):\n" +
                "\n".join([os.path.basename(d) for d in directories if d not in [d for _, msg in zip(directories, msg) if 'âœ…' in msg]]) +
                f"\nâŒ ä»¥ä¸‹ç›®å½•æ¸…ç†å¤±è´¥ ({len(error_messages)}):\n" +
                "\n".join([f"â€¢ {msg}" for msg in error_messages[:3]])
            )
        elif success_dirs == 0 and zero_dirs_count == 0 and len(error_messages) > 0:
            final_text = f"âŒ æ¸…ç†å¤±è´¥ï¼æœªæˆåŠŸæ¸…ç†ä»»ä½•ç›®å½•ã€‚\n" + "\n".join([f"â€¢ {msg}" for msg in error_messages[:3]])
        elif success_dirs == 0 and zero_dirs_count > 0 and len(error_messages) == 0:
            final_text = f"âš ï¸ æ‰€æœ‰ç›®å½•å‡æœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„æ–‡ä»¶ ({total_dirs})ã€‚"
        elif success_dirs == 0 and zero_dirs_count > 0 and len(error_messages) > 0:
            final_text = (
                f"âŒ æ¸…ç†å¤±è´¥ï¼æœªæˆåŠŸæ¸…ç†ä»»ä½•ç›®å½•ã€‚\n"
                f"âš ï¸ ä»¥ä¸‹ç›®å½•æœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„æ–‡ä»¶ ({zero_dirs_count}):\n" +
                "\n".join([os.path.basename(d) for d in directories if d not in [d for _, msg in zip(directories, msg) if 'âœ…' in msg]]) +
                f"\nâŒ ä»¥ä¸‹ç›®å½•æ¸…ç†å¤±è´¥ ({len(error_messages)}):\n" +
                "\n".join([f"â€¢ {msg}" for msg in error_messages[:3]])
            )
        else:
            final_text = f"âœ… éƒ¨åˆ†æ¸…ç†å®Œæˆï¼æˆåŠŸæ¸…ç† {total_files} ä¸ªå°æ–‡ä»¶ï¼Œæ¶‰åŠ {success_dirs} ä¸ªç›®å½•ã€‚"

        await processing_msg.edit_text(final_text)
    except Exception as e:
        logger.error(f"æ¸…ç†å‘½ä»¤å¼‚å¸¸: {str(e)}", exc_info=True)
        await processing_msg.edit_text(f"âŒ æ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)[:50]}")

@restricted
async def refresh_command(update: Update, context: ContextTypes.DEFAULT_TYPE, *, token: str) -> None:
    """åˆ·æ–° Alist æ–‡ä»¶åˆ—è¡¨"""
    refresh_url = BASE_URL.rstrip('/') + "/api/fs/list"
    headers = {"Authorization": token, "Content-Type": "application/json"}
    payload = {"path": context.bot_data.get('current_download_dir', ALIST_OFFLINE_DIRS[0]), "page": 1, "per_page": 0, "refresh": True}
    chat_id = update.effective_chat.id
    processing_msg = await update.message.reply_text("ğŸ”„ æ­£åœ¨åˆ·æ–° Al-ah...")

    try:
        loop = asyncio.get_running_loop()
        response = await loop.run_in_executor(
            None, lambda: requests.post(refresh_url, json=payload, headers=headers, timeout=30)
        )
        response.raise_for_status()
        result = response.json()

        if result.get("code") == 200:
            await processing_msg.edit_text("âœ… Alist åˆ·æ–°æˆåŠŸï¼")
        else:
            error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
            await processing_msg.edit_text(f"âŒ åˆ·æ–°å¤±è´¥: {error_msg}")
    except requests.exceptions.RequestException as e:
        logger.error(f"åˆ·æ–° Alist æ—¶å‡ºé”™: {str(e)}")
        await processing_msg.edit_text(f"âŒ åˆ·æ–°å¤±è´¥: ç½‘ç»œé”™è¯¯ ({str(e)[:50]})")
    except Exception as e:
        logger.error(f"åˆ·æ–° Alist æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}", exc_info=True)
        await processing_msg.edit_text(f"âŒ åˆ·æ–°å¤±è´¥: æœªçŸ¥é”™è¯¯ ({str(e)[:50]})")

@restricted
async def list_paths(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str) -> None:
    """åˆ—å‡ºæ‰€æœ‰ä¸‹è½½è·¯å¾„"""
    dirs = ALIST_OFFLINE_DIRS
    current_index = context.bot_data.get('current_download_dir_index', 0)
    if not dirs:
        await update.message.reply_text("No download directories configured.")
        return
    message = "ä¸‹è½½ç›®å½•åˆ—è¡¨:\n"
    for i, dir in enumerate(dirs, 1):
        message += f"{i}. {dir}\n"
    if 0 <= current_index < len(dirs):
        message += f"å½“å‰ç›®å½•: {current_index + 1}. {dirs[current_index]}"
    else:
        message += "å½“å‰ç›®å½•: Unknown"
    await update.message.reply_text(message)

@restricted
async def switch_path(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str) -> None:
    """åˆ‡æ¢ä¸‹è½½è·¯å¾„"""
    if len(context.args) != 1:
        await update.message.reply_text("é‡æ–°å‘é€æœ‰æ•ˆå‘½ä»¤: /switch <æ•°å­—>")
        return
    try:
        index = int(context.args[0]) - 1  # 1-based to 0-based
        dirs = ALIST_OFFLINE_DIRS
        if 0 <= index < len(dirs):
            context.bot_data['current_download_dir_index'] = index
            context.bot_data['current_download_dir'] = dirs[index]
            await update.message.reply_text(f"åˆ‡æ¢åˆ°ä¸‹è½½ç›®å½• {index + 1}: {dirs[index]}")
        else:
            await update.message.reply_text(f"æ•°å­—æ— æ•ˆ. è¯·é‡æ–°é€‰æ‹© 1 and {len(dirs)}")
    except ValueError:
        await update.message.reply_text("è¯·æä¾›ä¸€ä¸ªæœ‰æ•ˆçš„æ•°å­—.")

@restricted
async def reload_config(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str) -> None:
    global ALIST_OFFLINE_DIRS
    load_dotenv(override=True)  # æ”¹ä¸º override=True
    ALIST_OFFLINE_DIRS = [d.strip() for d in os.getenv("ALIST_OFFLINE_DIRS", "").split(",") if d.strip()]
    if not ALIST_OFFLINE_DIRS:
        await update.message.reply_text("é‡è½½å¤±è´¥:  .envé‡Œæ²¡æœ‰ä¸‹è½½ç›®å½•.")
        return
    context.bot_data['current_download_dir_index'] = 0
    context.bot_data['current_download_dir'] = ALIST_OFFLINE_DIRS[0]
    await update.message.reply_text(f"é‡è½½å®Œæˆ. å·²åŠ è½½ {len(ALIST_OFFLINE_DIRS)} ä¸ªä¸‹è½½ç›®å½•. å·²è‡ªåŠ¨åˆ‡æ¢åˆ°ç›®å½•: {ALIST_OFFLINE_DIRS[0]}")

# --- è‡ªåŠ¨æ¸…ç†å®šæ—¶ä»»åŠ¡ ---
async def auto_clean(context: ContextTypes.DEFAULT_TYPE):
    if CLEAN_INTERVAL_MINUTES == 0 or SIZE_THRESHOLD == 0:
        logger.info("è‡ªåŠ¨æ¸…ç†ä»»åŠ¡æœªå¯ç”¨")
        return
    token = ALIST_TOKEN
    if not token:
        logger.error("ALIST_TOKEN æœªè®¾ç½®ï¼Œè‡ªåŠ¨æ¸…ç†ä»»åŠ¡å¤±è´¥ã€‚")
        return

    chat_id = list(ALLOWED_USER_IDS)[0]
    processing_msg = await context.bot.send_message(chat_id=chat_id, text="ğŸ§¹ å¼€å§‹è‡ªåŠ¨æ¸…ç†ä»»åŠ¡...")

    try:
        current_dir = context.bot_data.get('current_download_dir', ALIST_OFFLINE_DIRS[0])
        deleted_files, msg = await cleanup_small_files(token, BASE_URL, current_dir)
        final_text = f"è‡ªåŠ¨æ¸…ç†å®Œæˆ\n{msg}"
        await processing_msg.edit_text(final_text)
    except Exception as e:
        logger.error(f"è‡ªåŠ¨æ¸…ç†ä»»åŠ¡å¼‚å¸¸: {str(e)}", exc_info=True)
        error_text = [
            "âŒ è‡ªåŠ¨æ¸…ç†è¿‡ç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯",
            f"é”™è¯¯ç±»å‹: {type(e).__name__}",
            f"è¯¦ç»†ä¿¡æ¯: {str(e)}"
        ]
        await processing_msg.edit_text("\n".join(error_text))

# --- ä¸»å‡½æ•° ---
def main() -> None:
    """å¯åŠ¨æœºå™¨äºº"""
    application = Application.builder().token(TELEGRAM_TOKEN).build()
    if ALIST_OFFLINE_DIRS:
        application.bot_data['current_download_dir_index'] = 0
        application.bot_data['current_download_dir'] = ALIST_OFFLINE_DIRS[0]
    else:
        logger.error("No download directories loaded.")
        sys.exit(1)

    # æ³¨å†Œå‘½ä»¤å¤„ç†ç¨‹åº
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("clean", clean_command))
    application.add_handler(CommandHandler("refresh", refresh_command))
    application.add_handler(CommandHandler("list_paths", list_paths))
    application.add_handler(CommandHandler("switch", switch_path))
    application.add_handler(CommandHandler("reload_config", reload_config))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, process_message))

    # å¯åŠ¨è‡ªåŠ¨æ¸…ç†ä»»åŠ¡
    job_queue = application.job_queue
    job_queue.run_repeating(auto_clean, interval=CLEAN_INTERVAL_MINUTES * 60, first=0)

    # å¯åŠ¨æœºå™¨äºº
    application.run_polling()

if __name__ == "__main__":
    main()
